# Read about the usethis package

usethis::use_usethis()

# open the .Rprofile and add the one liner generated by above code which is already copied to the clipboard and save the .Rprofile. This is to load usethis when we start R so we dont have to say usethis::use_package("readr") each time and instead we can just write use_package like below

#Restart the R using palette

source("\~/.Rprofile")

# Add the usethis package as a workflow dependancy

use_package("usethis", "suggests")

#Workflow dependency: suggets #Build dependancy: Imports #“imports”
corresponds to build dependencies, while “suggests” are equivalent to
what we call workflow dependencies.

#Connect to Github

#Add the gitcreds package as a workflow dependency.
use_package("gitcreds", "suggests")

usethis::use_github()

# style speciif R files

# Use package styler as a workflow dependency

use_package("styler", "suggests")

# style the file we’re currently working on using the “style active file” RStudio addin. We use that through the Command Palette (Ctrl-Shift-P) and typing “style file”, which should show the “Style active file” option. This is what we will do frequently throughout the course.

styler:::style_active_file()

# style, commit , push

# to style whole directory

styler::style_dir()

#qmd and python works together. Qaurto is better than Rmd

#style the markdown files at project level

#Go to tools, project options , R markdown: canonical visual: true,
#text; wrapping:column, ok

# open learning R file doc:learning,qmd

#Add soemthing to the the learning,qmd file and save, it will
#automatically insert proper spacing

# Open up the README.md file and do all the TODO items found inside the file as well as inside the TODO.md file. Save often and watch as the Markdown gets reformatted.

#6. creating automated pipelines

# Targets: Using targets to manage a pipeline

Target allows you to explicitly specify the outputs you want to create.
targets will then track them for you and know which output depends on
which as well as which ones need to be updated when you make changes to
your pipeline.

#Pipeline A pipeline can be any process where the steps between a start
and an end point are very clear, explicit, and concrete. These highly
distinct steps can be manual, human involved, or completely automated by
a robot or computer.

# 1. function based workflow: target uses this, and an powerful way to make workflows

# 2. script based workflow: old fashioned

use_package("targets")

# add targets to our project

targets::use_targets() \# creates a file called targets.R

Mostly, the_targets.R script contains comments and a basic set up to get
you started. But, notice the tar_target() function used at the end of
the script. There are two main arguments for it: name and command. The
way that targets works is similar to how you’d assign the output of a
function to an object, so:

```{r}
object_name <- function_in_command(input_arguments)
```

#Is the same as:

```{r}
tar_target(name = object_name, 
           command =function_in_command(input_arguments) )
```

What this means is that targets follows a “function-oriented” workflow,
not a “script-based” workflow. What’s the difference? In a
script-oriented workflow, each R file/script is run in a specific order.
As a result, you might end up with an R file that has code like:

```{r}
source("R/1-process-data.R")
source("R/2-basic-statistics.R")
source("R/3-create-plots.R") 
source("R/4-linear-regression.R")
```

#While in a function-oriented workflow, it might look more like:

```{r}
source("R/functions.R")
raw_data<- load_raw_data("file/path/data.csv")
processed_data<- process_data(raw_data)
basic_stats<-calculate_basic_statistics(processed_data)
simple_plot<-create_plot(processed_data)
model_results<-run_linear_reg(processed_data)
```

With the function-oriented workflow, each function takes an input and
contains all the code to create one result as its output. This could be,
for instance, a figure in a paper.

# To load tidyverse and tidymodels

```{r}
use_package("tidyverse", "depends")
```

we have 3 levels in the description file

-   

    1.  imports

-   

    2.  suggests

-   

    3.  depends

because they are metapackages they issue a warning without the above

# Open doc/learning.qmd

Tip: don't do giant commits, do commits often even the small commits

Create 3 outputs 1. descriptive statistics (table1) 2. plots 3. Manus

Make the function in learning.qmd, take it to functions.R file then
eventually move it to the targets file

# In the {r basic-stats} chunk

```{r}
# test in the leadning.qmd
lipidomics |>
  group_by(metabolite) |>
  summarise(across(value, list(mean = mean, sd = sd))) |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 1)))
```

# with palette insert a code chunk

```{r}
lipidomics %>% # or lipidomics |>
  group_by(metabolite) %>%
  summarise(across(
    value,
    list(
      mean = mean,
      sd = sd
    )
  )) %>%
  mutate(across(
    where(is.numeric),
    ~ round(.x, digits = 1)
  ))
```

## Make it more fancy and add Title and param etc

```{r}
#' Title "Descriptive statistics" 
#'
#' @param data 
#'
#' @return  “A data.frame/tibble.”
#'
descriptive_stats <- function(data) {
  data %>% dplyr::group_by(metabolite) %>%
    dplyr::summarise(across(
      value,
      list(
        mean = mean,
        sd = sd
      )
    )) %>%
    dplyr::mutate(across(
      where(is.numeric),
      ~ round(.x, digits = 1)
    ))
}
```

Now remove the code from learning.qmd and add this code to functions.R
and test it by running in console the following:

```{r}
descriptive_stats(lipidomics)
```

# Now Add the basic statistics as a step in the targets pipeline

```{r}

#edit targets.R and add the following and save


list(
  tar_target(
    name = lipidomics,
    command = readr::read_csv(here::here("data/lipidomics.csv"))
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

Now run in console

```{r}
targets::tar_make()
```

Output: - dispatched target df_stats_by_metabolite - completed target -
df_stats_by_metabolite \[0.13 seconds, 403 bytes\] - ended pipeline
\[0.78 seconds\]

# Tell target to keep track of our file

```{r}
list(
  tar_target(
    name = file,
    command = "data/lipidomics.csv",
    format = "file"
  ),
  tar_target(
    name = lipidomics,
    command = readr::read_csv(file, show_col_types = FALSE)
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

# Visualize the network

```{r}
targets::tar_visnetwork()
```

# To check what is changed

```{r}
targets::tar_outdated()
```

# start plotting

use_package("ggplot2")

Add to the learning.qmd

```{r}
ggplot(lipidomics, aes(x = value)) +
    geom_histogram() +
    facet_wrap(vars(metabolite), scales = "free")


# convert the plot into a function and move it inside functions.R

#' Title Create Plots
#'
#' @param data Lipidomics dataset
#'
#' @return Histograms of metabolites
#'
plot_distributions <- function(data) {
    data %>%
        ggplot2::ggplot(ggplot2::aes(x = value)) +
        ggplot2::geom_histogram() +
        ggplot2::facet_wrap(ggplot2::vars(metabolite), scales = "free")
        }
```

# Load packages required to define the pipeline in the targets file

```{r}
library(targets)
library(tarchetypes)
library(quarto)

# in the console

targets::tar_make()
targets::tar_visnetwork()
```

# Connect quarto_doc and learning.qmd

# read the targets package and read the data file from targets file

```{r setup}
targets::tar_config_set(store = here::here("_targets"))
library(tidyverse)
library(targets)
source(here::here("R/functions.R"))
lipidomics = tar_read(lipidomics)

## Results

tar_read(df_stats_by_metabolite)

tar_read(fig_metabolite_distribution)
```

# make target

```{r}
targets::tar_make()
```

#change the path of target.yaml file or ignore that file to upload to
github

```{r}
use_git_ignore("_targets.yaml")
```

# in the leaning.qmd file, make the table pretty by listing mean and sd values together

```{r}
tar_read(df_stats_by_metabolite) |>
    mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) |>
    select(Metabolite = metabolite, `Mean (SD)` = MeanSD) |>
    knitr::kable(caption = "Descriptive Statistics of the Metabolites")
```

```{r}
targets::tar_visnetwork()
targets::tar_make()

```

### Working with Tidymodels

Add tidymodels in console

```{r}
use_package("tidymodels", "depends") 
use_package("workflows")
use_package("recipes")
use_package("parsnip")
```

# update learning.qmd file

```{r}
library(tidymodels)


##Building models
log_reg_specs = logistic_reg() |>
    set_engine("glm")

log_reg_specs  

# Fix the data and multiple entry issue, normalize the data

# identify individuals with the same metabolote repeated more than once

lipidomics|> count(code, metabolite) |> filter(n>1)


# convert the data to wide format

lipidomics_wide <- lipidomics |>
  mutate(metabolite = snakecase::to_snake_case(metabolite)) |>
  pivot_wider(
    names_from = metabolite,
    values_from = value,
    values_fn = mean,
    names_prefix = "metabolite_"
  )
lipidomics_wide

convert both the “metabolite to snakecase” and “pivot to wider” code
into their own functions

column_values_to_snake_case <- function(data) {
  data |>
    dplyr::mutate(metabolite = snakecase::to_snake_case(metabolite))
}
lipidomics |>
  column_values_to_snake_case()


```

# Learning about the argument “curly-curly” ({{}})

E.g

```{r}

test_nse <- function(data, columns) {
  data |>
    dplyr::select({{ columns }})
}

lipidomics |>
  test_nse(class)

lipidomics |>
  test_nse(c(class, age))
```

# Now we hould be able to use curly-curly (combined with across()) to apply snakecase::to_snake_case() to columns of our choice.

# Note we dont have to specificy metabolite here

# make a function for converting values to snake_case and save it in functions.R

```{r}

column_values_to_snake_case <- function(data, columns) {
  data |>
    dplyr::mutate(dplyr::across({{ columns }}, snakecase::to_snake_case))
}


lipidomics |>
  column_values_to_snake_case(metabolite)

```

source("R/functions.R")

```{r}
lipidomics |>
  column_values_to_snake_case(metabolite) |>
   pivot_wider(
    names_from = metabolite,
    values_from = value,
    values_fn = mean,
    names_prefix = "metabolite_"
  )
```

# now make a function for pivot_wider and save it in functions.R

```{r}
#' Convert the metabolite long format into a wider one.
#'
#' @param data The lipidomics dataset.
#'
#' @return A wide data frame.
#'
metabolites_to_wider <- function(data) {
    data |>
        tidyr::pivot_wider(
            names_from = metabolite,
            values_from = value,
            values_fn = mean,
            names_prefix = "metabolite_"
        )
}
```

In the console, now use both functions together to save the data in wide
format, also saved the below in learning.qmd

```{r}
lipidomics_wide = lipidomics |>
  column_values_to_snake_case(metabolite) |>
  metabolites_to_wider()
```

#Using recipes to manage transformations

# Normalize data using recipes

There are many transformations we could use for the lipidomics dataset,
but we will use step_normalize() for this course.

```{r}
recipe(lipidomics_wide) |>
  update_role(metabolite_cholesterol, age, gender, new_role = "predictor") |>
  update_role(class, new_role = "outcome") |>
  step_normalize(starts_with("metabolite_"))
```

## add this to the functions.R file

```{r}

#' A transformation recipe to pre-process the data.
#'
#' @param data The lipidomics dataset.
#' @param metabolite_variable The column of the metabolite variable.
#'
#' @return
#'
create_recipe_spec <- function(data, metabolite_variable) {
    recipes::recipe(data) |>
        recipes::update_role({{ metabolite_variable }}, age, gender, new_role = "predictor") |>
        recipes::update_role(class, new_role = "outcome") |>
        recipes::step_normalize(tidyselect::starts_with("metabolite_"))
}

```

TEst it in the learning.qmd file

```{r}
recipe_specs <- lipidomics_wide |>
  create_recipe_spec(metabolite_cholesterol)
recipe_specs

```

## Using the workflows package to combine log_reg_specs and recipe_specs

All model workflows need to start with workflow(), followed by two main
functions: add_model() and add_recipe()

```{r}
workflow() |>
  add_model(log_reg_specs) |>
  add_recipe(recipe_specs)
```

## create a function using the above and save it in functions.R

```{r}
#' Create a workflow object of the model and transformations.
#'
#' @param model_specs The model specs
#' @param recipe_specs The recipe specs
#'
#' @return A workflow object
#'
create_model_workflow <- function(model_specs, recipe_specs) {
  workflows::workflow() |>
    workflows::add_model(model_specs) |>
    workflows::add_recipe(recipe_specs)
}
```

## Do the modeling and test in learning.qmd

```{r}
model_workflow <- create_model_workflow(
  logistic_reg() |>
    set_engine("glm"),
  lipidomics_wide |>
    create_recipe_spec(metabolite_cholesterol)
)
model_workflow

# Fit the model
fitted_model <- model_workflow |>
  fit(lipidomics_wide)
fitted_model

# get the model estimates

fitted_model |>
  extract_fit_parsnip()



```

## use the package broom and tidy

```{r}
# in the console
use_package("broom")

# use function tidy in learning.qmd
fitted_model |>
  extract_fit_parsnip() |>
  tidy(exponentiate = TRUE)



```

## Covert the tidy model as a function in functions.R

```{r}
#' Create a tidy output of the model results.
#'
#' @param workflow_fitted_model The model workflow object that has been fitted.
#'
#' @return A data frame.
#'
tidy_model_output <- function(workflow_fitted_model) {
    workflow_fitted_model |>
        workflows::extract_fit_parsnip() |>
        broom::tidy(exponentiate = TRUE)
}
```

# test it in learning.qmd

```{r}

fitted_model |>
  tidy_model_output()

```
