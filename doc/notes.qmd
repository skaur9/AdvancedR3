# Read about the usethis package

usethis::use_usethis()

# open the .Rprofile and add the one liner generated by above code which is already copied to the clipboard and save the .Rprofile. This is to load usethis when we start R so we dont have to say usethis::use_package("readr") each time and instead we can just write use_package like below

#Restart the R using palette

source("\~/.Rprofile")

# Add the usethis package as a workflow dependancy

use_package("usethis", "suggests")

#Workflow dependency: suggets #Build dependancy: Imports #“imports”
corresponds to build dependencies, while “suggests” are equivalent to
what we call workflow dependencies.

#Connect to Github

#Add the gitcreds package as a workflow dependency.
use_package("gitcreds", "suggests")

usethis::use_github()

# style speciif R files

# Use package styler as a workflow dependency

use_package("styler", "suggests")

# style the file we’re currently working on using the “style active file” RStudio addin. We use that through the Command Palette (Ctrl-Shift-P) and typing “style file”, which should show the “Style active file” option. This is what we will do frequently throughout the course.

styler:::style_active_file()

# style, commit , push

# to style whole directory

styler::style_dir()

#qmd and python works together. Qaurto is better than Rmd

#style the markdown files at project level

#Go to tools, project options , R markdown: canonical visual: true,
#text; wrapping:column, ok

# open learning R file doc:learning,qmd

#Add soemthing to the the learning,qmd file and save, it will
#automatically insert proper spacing

# Open up the README.md file and do all the TODO items found inside the file as well as inside the TODO.md file. Save often and watch as the Markdown gets reformatted.

#6. creating automated pipelines

# Targets: Using targets to manage a pipeline

Target allows you to explicitly specify the outputs you want to create.
targets will then track them for you and know which output depends on
which as well as which ones need to be updated when you make changes to
your pipeline.

#Pipeline A pipeline can be any process where the steps between a start
and an end point are very clear, explicit, and concrete. These highly
distinct steps can be manual, human involved, or completely automated by
a robot or computer.

# 1. function based workflow: target uses this, and an powerful way to make workflows

# 2. script based workflow: old fashioned

use_package("targets")

# add targets to our project

targets::use_targets() \# creates a file called targets.R

Mostly, the_targets.R script contains comments and a basic set up to get
you started. But, notice the tar_target() function used at the end of
the script. There are two main arguments for it: name and command. The
way that targets works is similar to how you’d assign the output of a
function to an object, so:

```{r}
object_name <- function_in_command(input_arguments)
```

#Is the same as:

```{r}
tar_target(name = object_name, 
           command =function_in_command(input_arguments) )
```

What this means is that targets follows a “function-oriented” workflow,
not a “script-based” workflow. What’s the difference? In a
script-oriented workflow, each R file/script is run in a specific order.
As a result, you might end up with an R file that has code like:

```{r}
source("R/1-process-data.R")
source("R/2-basic-statistics.R")
source("R/3-create-plots.R") 
source("R/4-linear-regression.R")
```

#While in a function-oriented workflow, it might look more like:

```{r}
source("R/functions.R")
raw_data<- load_raw_data("file/path/data.csv")
processed_data<- process_data(raw_data)
basic_stats<-calculate_basic_statistics(processed_data)
simple_plot<-create_plot(processed_data)
model_results<-run_linear_reg(processed_data)
```

With the function-oriented workflow, each function takes an input and
contains all the code to create one result as its output. This could be,
for instance, a figure in a paper.

# To load tidyverse and tidymodels

```{r}
use_package("tidyverse", "depends")
```

we have 3 levels in the description file

-   

    1.  imports

-   

    2.  suggests

-   

    3.  depends

because they are metapackages they issue a warning without the above

# Open doc/learning.qmd

Tip: don't do giant commits, do commits often even the small commits

Create 3 outputs 1. descriptive statistics (table1) 2. plots 3. Manus

Make the function in learning.qmd, take it to functions.R file then
eventually move it to the targets file

# In the {r basic-stats} chunk

```{r}
# test in the leadning.qmd
lipidomics |>
  group_by(metabolite) |>
  summarise(across(value, list(mean = mean, sd = sd))) |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 1)))
```

# with palette insert a code chunk

```{r}
lipidomics %>% # or lipidomics |>
  group_by(metabolite) %>%
  summarise(across(
    value,
    list(
      mean = mean,
      sd = sd
    )
  )) %>%
  mutate(across(
    where(is.numeric),
    ~ round(.x, digits = 1)
  ))
```

## Make it more fancy and add Title and param etc

```{r}
#' Title "Descriptive statistics" 
#'
#' @param data 
#'
#' @return  “A data.frame/tibble.”
#'
descriptive_stats <- function(data) {
  data %>% dplyr::group_by(metabolite) %>%
    dplyr::summarise(across(
      value,
      list(
        mean = mean,
        sd = sd
      )
    )) %>%
    dplyr::mutate(across(
      where(is.numeric),
      ~ round(.x, digits = 1)
    ))
}
```

Now remove the code from learning.qmd and add this code to functions.R
and test it by running in console the following:

```{r}
descriptive_stats(lipidomics)
```

# Now Add the basic statistics as a step in the targets pipeline

```{r}

#edit targets.R and add the following and save


list(
  tar_target(
    name = lipidomics,
    command = readr::read_csv(here::here("data/lipidomics.csv"))
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

Now run in console

```{r}
targets::tar_make()
```

Output: - dispatched target df_stats_by_metabolite - completed target -
df_stats_by_metabolite \[0.13 seconds, 403 bytes\] - ended pipeline
\[0.78 seconds\]

# Tell target to keep track of our file

```{r}
list(
  tar_target(
    name = file,
    command = "data/lipidomics.csv",
    format = "file"
  ),
  tar_target(
    name = lipidomics,
    command = readr::read_csv(file, show_col_types = FALSE)
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

# Visualize the network

```{r}
targets::tar_visnetwork()
```

# To check what is changed

```{r}
targets::tar_outdated()
```

# start plotting

use_package("ggplot2")

Add to the learning.qmd

```{r}
ggplot(lipidomics, aes(x = value)) +
    geom_histogram() +
    facet_wrap(vars(metabolite), scales = "free")


# convert the plot into a function and move it inside functions.R

#' Title Create Plots
#'
#' @param data Lipidomics dataset
#'
#' @return Histograms of metabolites
#'
plot_distributions <- function(data) {
    data %>%
        ggplot2::ggplot(ggplot2::aes(x = value)) +
        ggplot2::geom_histogram() +
        ggplot2::facet_wrap(ggplot2::vars(metabolite), scales = "free")
        }
```

# Load packages required to define the pipeline in the targets file

```{r}
library(targets)
library(tarchetypes)
library(quarto)

# in the console

targets::tar_make()
targets::tar_visnetwork()
```

# Connect quarto_doc and learning.qmd

# read the targets package and read the data file from targets file

```{r setup}
targets::tar_config_set(store = here::here("_targets"))
library(tidyverse)
library(targets)
source(here::here("R/functions.R"))
lipidomics = tar_read(lipidomics)

## Results

tar_read(df_stats_by_metabolite)

tar_read(fig_metabolite_distribution)
```

# make target

```{r}
targets::tar_make()
```

#change the path of target.yaml file or ignore that file to upload to
github
